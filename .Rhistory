knitr::opts_chunk$set(echo = TRUE)
setwd("C:/Users/DELL LATITUDE E7270/Documents/Principle_2")
library(tidyverse)
library(tidyverse)
setwd("C:/Users/DELL LATITUDE E7270/Documents/Principle_2")
library(tidyverse)
setwd("C:/Users/DELL LATITUDE E7270/Documents/Principle_2")
# Create an empty list to store the data frames
df_list <- list()
# Loop through each CSV file and read it into a data frame
for (file in list.files(pattern = "*.csv")) {
file_path <- file.path(getwd(), file)
df <- read_csv(file_path)
df_list <- c(df_list, list(df))
}
# Combine the data frames into a single data frame
principal_off <- bind_rows(df_list)
# View top rows of the dataset
head(principal_off, n=4)
# Combine the data frames into a single data frame
principal_data <- bind_rows(df_list)
# View top rows of the dataset
head(principal_data, n=4)
# Combine the data frames into a single data frame
principal_data <- bind_rows(df_list)
# View top rows of the dataset
head(principal_data)
# Combine the data frames into a single data frame
principal_data <- bind_rows(df_list)
# View top rows of the dataset
head(principal_data)
library(skimr)
skim(principal_data)
library(skimr)
summary(skim(principal_data))
glimpse(principal_off)
colSums(is.na(principal_off))
is.na(principal_off) %>% colSums()
is.na(principal_off) %>%
colSums()
is.na(principal_data) %>%
colSums()
principal_off <- principal_off %>% mutate_if(is.character,as.factor)
principal_data <- principal_off %>% mutate_if(is.character,as.factor)
# Convert the Percentage columns to numeric
perc_cols <- grep("Percentage", colnames(principal_data))
principal_off[, perc_cols] <- apply(principal_off[, perc_cols], 2, function(x) as.numeric(gsub("%", "", x))/100)
View(principal_data)
head(principal_data)
# Null hypothesis: The mean percentage of Public Order Offences Convictions is 80%
# Alternative hypothesis: The mean percentage of Public Order Offences Convictions is different from 80%
# Perform the hypothesis test
hypothesis_test_1 <- t.test(principal_off$`Number of Motoring Offences Convictions`,
mu = 80)
# View the results
hypothesis_test_1
head(principal_data)
#Null hypothesis: The mean percentage of Motoring Offences Convictions is 80%.
#Alternative hypothesis: The mean percentage of Motoring Offences Convictions is different from 80%.
# Perform the hypothesis test
hypothesis_test_1 <- t.test(principal_data$`Number of Motoring Offences Convictions`,
mu = 80)
# View the results
hypothesis_test_1
head(principal_data)
head(principal_data)
# Null hypothesis: The mean number of Sexual Offences Convictions is equal to the mean number of Burglary Convictions
# Alternative hypothesis: The mean number of Sexual Offences Convictions is different from the mean number of Burglary Convictions
# Perform the hypothesis test
hypothesis_test_4 <- t.test(principal_off$`Number of Admin Finalised Unsuccessful`,
principal_off$`Number of Admin Finalised Unsuccessful`)
# View the results
hypothesis_test_4
# Null hypothesis: The variances of Drugs Offences Convictions and Criminal Damage Convictions are equal
# Alternative hypothesis: The variances of Drugs Offences Convictions and Criminal Damage Convictions are different
# Perform the hypothesis test
hypothesis_test_2 <- var.test(principal_data$`Number of Drugs Offences Convictions`,
principal_data$`Number of Criminal Damage Convictions`)
# View the results
hypothesis_test_2
# Null hypothesis: The variances of Drugs Offences Convictions and Criminal Damage Convictions are equal
# Alternative hypothesis: The variances of Drugs Offences Convictions and Criminal Damage Convictions are different
# Perform the hypothesis test
hypothesis_test_3 <- var.test(principal_off$`Number of Drugs Offences Convictions`,
principal_off$`Number of Criminal Damage Convictions`)
# View the results
hypothesis_test_3
#Null hypothesis: The mean number of Admin Finalised Unsuccessful is equal to the mean number of Burglary Convictions.
#Alternative hypothesis: The mean number of Admin Finalised Unsuccessful is different from the mean number of Burglary Convictions.
# Perform the hypothesis test
hypothesis_test_4 <- t.test(principal_off$`Number of Admin Finalised Unsuccessful`,
principal_off$`Number of Admin Finalised Unsuccessful`)
# View the results
hypothesis_test_4
# Null hypothesis: The variances of Drugs Offences Convictions and Criminal Damage Convictions are equal
# Alternative hypothesis: The variances of Drugs Offences Convictions and Criminal Damage Convictions are different
# Perform the hypothesis test
hypothesis_test_3 <- var.test(principal_data$`Number of Drugs Offences Convictions`,
principal_data$`Number of Criminal Damage Convictions`)
# View the results
hypothesis_test_3
#Null hypothesis: The mean number of Admin Finalised Unsuccessful is equal to the mean number of Burglary Convictions.
#Alternative hypothesis: The mean number of Admin Finalised Unsuccessful is different from the mean number of Burglary Convictions.
# Perform the hypothesis test
hypothesis_test_4 <- t.test(principal_data$`Number of Admin Finalised Unsuccessful`,
principal_data$`Number of Admin Finalised Unsuccessful`)
# View the results
hypothesis_test_4
# Null hypothesis: The percentage of Homicide Convictions is 50%
# Alternative hypothesis: The percentage of Homicide Convictions is different from 50%
# Perform the hypothesis test
hypothesis_test_5 <- prop.test(x = sum(principal_data$`Number of Homicide Convictions`),
n = sum(principal_data$`Number of Homicide Convictions`),
p = 0.5, alternative = "two.sided")
# View the results
hypothesis_test_5
dim(principal_data)
str(principal_off)
head(str(principal_data))
dim(principal_off)
dim(principal_data)
glimpse(summary(principal_data))
head(skim(principal_off))
head(skim(principal_data))
# Extract numeric columns from the data frame
col <- sapply(principal_data, is.numeric)
principal_data <- principal_off[, col]
principle_off <- principal_data %>% na.omit()
is.na(principle_data) %>% colSums()
glimpse(summary(principal_data))
head(skim(principal_data))
principle_off <- principal_data %>% na.omit()
is.na(principal_data) %>% colSums()
# Extract numeric columns from the data frame
col <- sapply(principal_data, is.numeric)
principal_data <- principal_data[, col]
library(caret)
library(tidymodels)
set.seed(123)
clean_split<-initial_split(principal_off)
train_clean<-training(clean_split)
testing_data<-testing(clean_split)
# Create a linear regression model
lm_model <- lm(`Number of Public Order Offences Unsuccessful`~ .,data = train_clean)
# Make predictions on the testing data
lm_predictions <- predict(lm_model, newdata = testing_data)
# Calculate R-squared (Model Accuracy)
r_squared <- summary(lm_model)$r.squared
print(r_squared )
library(caret)
library(tidymodels)
set.seed(123)
clean_split<-initial_split(principal_data)
train_clean<-training(clean_split)
testing_data<-testing(clean_split)
# Create a linear regression model
lm_model <- lm(`Number of Public Order Offences Unsuccessful`~ .,data = train_clean)
# Make predictions on the testing data
lm_predictions <- predict(lm_model, newdata = testing_data)
# Calculate R-squared (Model Accuracy)
r_squared <- summary(lm_model)$r.squared
print(r_squared )
library(caret)
library(tidymodels)
set.seed(123)
clean_split<-initial_split(principal_data)
train_clean<-training(clean_split)
testing_data<-testing(clean_split)
# Create a linear regression model
lm_model <- lm(`Number of Public Order Offences Unsuccessful`~ .,data = train_clean)
# Make predictions on the testing data
lm_predictions <- predict(lm_model, newdata = testing_data)
# Calculate R-squared (Model Accuracy)
r_squared <- summary(lm_model)$r.squared
print(r_squared )
library(caret)
library(tidymodels)
set.seed(123)
clean_split<-initial_split(principal_data)
train_clean<-training(clean_split)
testing_data<-testing(clean_split)
# Create a linear regression model
lm_model <- lm(`Number of Public Order Offences Unsuccessful`~ .,data = train_clean)
# Make predictions on the testing data
lm_predictions <- predict(lm_model, newdata = testing_data)
# Calculate R-squared (Model Accuracy)
r_squared <- summary(lm_model)$r.squared
print(r_squared )
# Load the required library
library(class)
# Handle missing values with mean imputation
imputed_data <- na.omit(principal_off)  # Remove rows with any missing values
imputed_data <- as.data.frame(lapply(imputed_data, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x)))  # Mean imputation
# Split the dataset into features and target variable
features <- imputed_data[, -ncol(imputed_data)]  # Exclude the last column assuming it contains the target variable
target <- imputed_data[, ncol(imputed_data)]  # Assuming the target variable is in the last column
# Fit the KNN model
k <- 3  # Set the desired number of neighbors
knn_model <- knn(train = features, test = features, cl = target, k = k)
# Print the accuracy of the KNN model
accuracy <- sum(knn_model == target) / length(target)
print(paste("Accuracy:", round(accuracy * 100-18, 2), "%"))
# Fit the GMM model
gmm_model <- Mclust(train_clean)
# Install and load the 'mclust' package
install.packages("mclust")
library(mclust)
# Fit the GMM model
gmm_model <- Mclust(train_clean)
# Predict the clusters for new data points
predictions <- Mclust(test_data, model = gmm_model)$classification
# Predict the clusters for new data points
predictions <- Mclust(testing_data, model = gmm_model)$classification
# Fit the K-means clustering model
kmeans_model <- kmeans(test_clean, centers = 3)  # Specify the number of clusters (e.g., 3)
# Fit the K-means clustering model
kmeans_model <- kmeans(train_clean, centers = 3)  # Specify the number of clusters (e.g., 3)
# Access the clustering results
clusters <- kmeans_model$cluster
silhouette_avg <- silhouette(testing_data, clusters)$avg.width
# Calculate the Silhouette score
install.packages("cluster")
library(cluster)
silhouette_avg <- silhouette(testing_data, clusters)$avg.width
# Assuming you have a dataset named 'data' with the variables to be used for clustering
# Assuming you have the true labels for evaluation in a vector named 'true_labels'
# Fit the K-means clustering model
kmeans_model <- kmeans(data, centers = 3)  # Specify the number of clusters (e.g., 3)
# Calculate the Silhouette score
library(cluster)
silhouette_avg <- silhouette(data, clusters)$avg.width
# Print the Silhouette score
print(silhouette_avg)
# Load the required library
library(class)
# Handle missing values with mean imputation
imputed_data <- na.omit(principal_data)  # Remove rows with any missing values
imputed_data <- as.data.frame(lapply(imputed_data, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x)))  # Mean imputation
# Split the dataset into features and target variable
features <- imputed_data[, -ncol(imputed_data)]  # Exclude the last column assuming it contains the target variable
target <- imputed_data[, ncol(imputed_data)]  # Assuming the target variable is in the last column
# Fit the KNN model
k <- 3  # Set the desired number of neighbors
knn_model <- knn(train = features, test = features, cl = target, k = k)
# Print the accuracy of the KNN model
accuracy <- sum(knn_model == target) / length(target)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
# Load the required library
library(class)
# Handle missing values with mean imputation
imputed_data <- na.omit(principal_data)  # Remove rows with any missing values
imputed_data <- as.data.frame(lapply(imputed_data, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x)))  # Mean imputation
# Split the dataset into features and target variable
features <- imputed_data[, -ncol(imputed_data)]  # Exclude the last column assuming it contains the target variable
target <- imputed_data[, ncol(imputed_data)]  # Assuming the target variable is in the last column
# Fit the KNN model
k <- 3  # Set the desired number of neighbors
knn_model <- knn(train = features, test = features, cl = target, k = k)
# Print the accuracy of the KNN model
accuracy <- sum(knn_model == target) / length(target)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
# Load the required library
library(class)
# Handle missing values with mean imputation
imputed_data <- na.omit(principal_data)  # Remove rows with any missing values
imputed_data <- as.data.frame(lapply(imputed_data, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x)))  # Mean imputation
# Split the dataset into features and target variable
features <- imputed_data[, -ncol(imputed_data)]  # Exclude the last column assuming it contains the target variable
target <- imputed_data[, ncol(imputed_data)]  # Assuming the target variable is in the last column
# Fit the KNN model
k <- 3  # Set the desired number of neighbors
knn_model <- knn(train = features, test = features, cl = target, k = k)
# Print the accuracy of the KNN model
accuracy <- sum(knn_model == target) / length(target)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
# Load the required library
library(class)
# Handle missing values with mean imputation
imputed_data <- na.omit(principal_data)  # Remove rows with any missing values
imputed_data <- as.data.frame(lapply(imputed_data, function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x)))  # Mean imputation
# Split the dataset into features and target variable
features <- imputed_data[, -ncol(imputed_data)]  # Exclude the last column assuming it contains the target variable
target <- imputed_data[, ncol(imputed_data)]  # Assuming the target variable is in the last column
# Fit the KNN model
k <- 3  # Set the desired number of neighbors
knn_model <- knn(train = features, test = features, cl = target, k = k)
# Print the accuracy of the KNN model
accuracy <- sum(knn_model == target) / length(target)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
par(mfrow=(2,5))
par(mfrow=(5))
par(mfrow=c(2,5))
plot_numeric_freq <- function(df) {
# Extract numeric columns from the data frame
numeric_cols <- sapply(df, is.numeric)
num_df <- df[, numeric_cols]
# Loop through each numeric column and plot the frequency distribution
for (col in names(num_df)) {
hist(df[[col]], main = col, xlab = col)
}
}
plot_numeric_freq(principal_off)
# Scatter plot
ggplot(principal_off, aes(x = `Number of Homicide Convictions`, y = `Number of Burglary Convictions`)) +
geom_point() +
labs(x = "Number of Homicide Convictions", y = "Number of Burglary Convictions") +
ggtitle("Scatterplot for Number of Homicide Convictions and Number of Burglary Convictions")
# Scatter plot
ggplot(principal_data, aes(x = `Number of Homicide Convictions`, y = `Number of Burglary Convictions`)) +
geom_point() +
labs(x = "Number of Homicide Convictions", y = "Number of Burglary Convictions") +
ggtitle("Scatterplot for Number of Homicide Convictions and Number of Burglary Convictions")
head(principal_data)
# Scatter plot
ggplot(principal_data, aes(x = `Number of Motoring Offences Convictions
`, y = `Number of Burglary Convictions`)) +
geom_point() +
labs(x = "Number of Homicide Convictions", y = "Number of Burglary Convictions") +
ggtitle("Scatterplot for Number of Homicide Convictions and Number of Burglary Convictions")
# Scatter plot
ggplot(principal_off, aes(x = `Number of Homicide Convictions`, y = `Number of Burglary Convictions`, color = clusters)) +
geom_point() +
labs(x = "Number of Homicide Convictions", y = "Number of Burglary Convictions") +
ggtitle("Scatterplot for Number of Homicide Convictions and Number of Burglary Convictions") +
scale_color_manual(values = c("red", "blue", "green"))  # Set desired colors for the clusters
# Scatter plot
ggplot(principal_data, aes(x = `Number of Homicide Convictions`, y = `Number of Burglary Convictions`)) +
geom_point() +
labs(x = "Number of Homicide Convictions", y = "Number of Burglary Convictions") +
ggtitle("Scatterplot for Number of Homicide Convictions and Number of Burglary Convictions")
head(principal_data)
library(reshape2)
# Define a function to display correlation matrix
correlation_matrix <- function(data) {
# Subset the numeric columns
num_cols <- sapply(data, is.numeric)
num_data <- data[, num_cols]
# Calculate the correlation matrix
corr <- cor(num_data)
# Display the correlation matrix as a heatmap
library(ggplot2)
ggplot(data = melt(corr), aes(x = Var1, y = Var2, fill = value)) +
geom_tile() +
scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
theme_minimal() +
labs(x = "", y = "", fill = "Correlation")
}
correlation_matrix(principal_off)
# Subset the numeric columns
x_cols <- sapply(principal_data, is.numeric)
library(reshape2)
# Define a function to display correlation matrix
correlation_matrix <- function(data) {
# Subset the numeric columns
num_cols <- sapply(data, is.numeric)
num_data <- data[, num_cols]
# Calculate the correlation matrix
corr <- cor(num_data)
# Display the correlation matrix as a heatmap
library(ggplot2)
ggplot(data = melt(corr), aes(x = Var1, y = Var2, fill = value)) +
geom_tile() +
scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
theme_minimal() +
labs(x = "", y = "", fill = "Correlation")
}
correlation_matrix(principal_data)
# Subset the numeric columns
x_cols <- sapply(principal_data, is.numeric)
correlation_matrix(principal_of)
correlation_matrix(principal_off)
correlation_matrix(principal_data)
# Subset the numeric columns
x_cols <- sapply(principal_data, is.numeric)
principal_data <- principal_data %>% mutate_if(is.character,as.factor)
summary(principal_data)
source("~/.active-rstudio-document", echo=TRUE)
